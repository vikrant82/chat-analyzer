---
description: Common issues and debugging strategies for the Chat Analyzer project
---
# Debugging Guide

## Common Issues and Solutions

### Frontend Issues

#### "Start Chat" Button Not Enabling (Mobile)
**Symptom**: Button stays disabled even after selecting all options  
**Cause**: Library instance not accessible via DOM property  
**Solution**: Use stored instance getter functions  
**See**: [ui.js](mdc:static/js/ui.js) `getFlatpickrInstance()`

#### Checkbox Not Working
**Symptom**: Backend receives `enabled: true` even when unchecked  
**Cause**: Hardcoded value in payload OR `checked` attribute in HTML  
**Solution**: 
- Remove `checked` from HTML
- Read from `checkbox.checked` in JavaScript
- Store as strings in localStorage

### Backend Performance Issues

#### Sequential Instead of Parallel Execution
**Symptom**: Logs show chunks completing one after another  
**Cause**: Synchronous blocking calls (like `requests.get()`)  
**Solution**: Wrap in `asyncio.to_thread()`  
**Location**: [webex_client.py](mdc:clients/webex_client.py) line 256

#### Message Duplication
**Symptom**: Same messages appear multiple times  
**Cause**: Chunks with overlapping time boundaries  
**Solution**: 
- Set `params['before'] = range_end_utc` for each chunk
- Implement deduplication with `seen_ids` set
**See**: [webex_client.py](mdc:clients/webex_client.py) lines 303-311

#### Stuck at 1000 Messages
**Symptom**: Only 1000 messages fetched, more exist  
**Cause**: Pagination stopping too early  
**Solution**: Change `if len(batch) < 2` to `if len(batch) < 1000`  
**See**: [webex_client.py](mdc:clients/webex_client.py) line 274

#### Fetching Wrong Time Window
**Symptom**: Chunks fetch from "now" instead of chunk end date  
**Cause**: Not setting `before` parameter correctly  
**Solution**: Always set `params['before'] = range_end_utc`  
**See**: [webex_client.py](mdc:clients/webex_client.py) line 249

### Telegram-Specific Issues

#### "Database is Locked" Errors
**Symptom**: Multiple "sqlite3.OperationalError: database is locked"  
**Cause**: Multiple TelegramClient instances accessing same session file  
**Solution**: Use single shared client for all parallel operations  
**See**: [telegram_client.py](mdc:clients/telegram_client.py) lines 240-420

#### Rate Limiting
**Symptom**: "Sleeping for 30s on GetHistoryRequest flood wait"  
**Expected**: This is normal Telegram API protection  
**Action**: No fix needed - parallelization is working so well it's hitting rate limits!  
**Workaround**: Reduce `max_concurrent_fetches` if rate limits are too aggressive

## Logging Best Practices

### Add Detailed Logs for Debugging

```python
logger.info(f"Range {start}-{end}: Batch #{count} received {len(batch)} messages, oldest={oldest}")
logger.info(f"After deduplication: {unique} unique from {total} total")
logger.info(f"Fetching {n} range(s) with max {concurrent} concurrent requests")
```

### Log Levels
- `logger.info()`: Normal operations, progress updates
- `logger.debug()`: Detailed debugging (use for batch details)
- `logger.warning()`: Recoverable issues (corrupted cache)
- `logger.error()`: Failures that should be investigated

## Testing Checklist

When implementing parallelization:
1. ✅ Test with small range (< 7 days) - should use single fetch
2. ✅ Test with medium range (14-21 days) - should split into 2-3 chunks
3. ✅ Test with large range (30+ days) - should split and parallelize
4. ✅ Test with cache disabled - should still chunk appropriately
5. ✅ Test with spotty cache (some days cached) - should handle gaps
6. ✅ Verify no duplicate messages in results
7. ✅ Verify all messages fetched (not stuck at API limits)
8. ✅ Check logs show parallel execution (simultaneous start times)
9. ✅ Monitor for API errors (503, rate limiting)
10. ✅ Test on mobile devices for UI changes
